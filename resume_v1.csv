Company,Description,Technologies,Role,Impact
Adyton,"Built and launched Adyton’s data platform on AWS from scratch. Designed ingestion of application events via PostHog, storing them in a data lake with S3, Glue, and Athena to enable behavioral analytics on screen engagement and retention. Implemented AWS Lambda integrations to bring changing product purchase lists from Google Sheets into the data lake, ensuring unified data availability. Added a feature flag and A/B testing framework using AppConfig to support product experimentation and iteration.","AWS S3, AWS Glue, AWS Athena, AWS Lambda, AWS AppConfig, PostHog",Data Platform Engineer,Enabled product and investor analytics leading to continued funding for the startup; supported internal product decisions with data-driven insights
Cloudelligent,"Led Cloudelligent’s Data and MLOps platform as a senior data engineer, translating client technical and business requirements into robust architectures and POCs that scaled to production applications. Designed and implemented data pipelines using S3, Glue, Athena, and Lambda, and built machine learning pipelines with SageMaker. Integrated generative AI capabilities through Amazon Bedrock, Q Business, and LangChain to support advanced ML and chatbot use cases. Provided pre-sales architectural guidance, proposals, and mentorship, including training materials for internal engineering staff.","AWS S3, Glue, Athena, Lambda, SageMaker, Bedrock, Amazon Q Business, LangChain",Data Engineer / Head of Data and MLOps Platform,"Closed high-profile contracts for Cloudelligent (including Red Bull), drove successful transitions of POCs into production systems, and mentored engineering staff to accelerate delivery velocity."
Compliance.ai,"Developed and deployed a configuration-based NLP pipeline for processing and summarizing legislative documents, enabling downstream regulatory updates for fintech stakeholders. Built a PDF ingestion and parsing system using docling, BeautifulSoup, and Tesseract, and trained a custom NLP model on legislature data using spaCy and SageMaker. Designed scalable data storage on S3, handling high-volume document ingestion to produce automated categorization and summarization in a pre-LLM environment.","AWS S3, SageMaker, spaCy, BeautifulSoup, Tesseract, docling",Senior Data Engineer,"Enabled fintech clients (including BlackRock) to receive timely, automated updates on state legislative changes, replacing slow manual workflows with a consistent, scalable pipeline."
iLovePDF,"Architected and delivered a real-time analytics platform to ingest clickstream data for iLovePDF. Designed and implemented streaming data pipelines using Kinesis and Firehose to land events into S3 and further store aggregated metrics in Timestream. Integrated Grafana dashboards to visualize key user behaviors in real time, enabling churn analysis and conversion optimization efforts to support product development.","AWS Kinesis, Kinesis Firehose, AWS S3, AWS Timestream, Grafana",Senior Data Engineer,"Delivered actionable, real-time insights into user behavior that directly informed website feature development and conversion tactics, improving data-driven product decisions."
Riot Games,"Served as Senior Data Engineer supporting Riot Games’ internal finance and data engineering teams by developing robust monitoring and forecasting pipelines. Optimized high-cost Databricks ETL jobs, reducing infrastructure spending by $1.3 million annually. Designed and built a code review tool powered by LangChain, GPT-4, and FAISS, which won first place at a company hackathon and was adopted by the Riot Data organization for ongoing code quality improvements.","AWS, Databricks, Airflow, LangChain, GPT-4, FAISS",Senior Data Engineer,"Reduced annual data pipeline costs by $1.3 million, accelerated finance team reporting workflows, and introduced generative AI code review automation still in active use."
Torchlight,"Designed and implemented an intelligence platform for ingesting and cross-referencing geospatial data from multiple third-party data vendors with web-based signals, including cryptocurrency transaction metadata and suspicious domain traffic. Integrated Google Locations API for reverse geocoding and MapBox for end-user visualizations. Additionally, built an internal GeoJSON auditing and polygon-redrawing tool using Leaflet, which streamlined location data curation and remains in production use today.","Google Locations API, MapBox, Leaflet, GeoJSON, Python, React, FastAPI, Google BigQuery",Senior Geospatial Data Engineer,"Enabled analysts to more efficiently analyze potential threat networks by combining location-based signals with web data, and improved data quality for the intelligence platform through robust, user-friendly internal tooling."
Mindex,"Served as Senior Data Engineer within Mindex’s Data and AI/ML practice, working directly with healthcare and emerging markets clients from pre-sales through production. Designed architectural frameworks that supported both proof-of-concept and production-ready deployments, including an AI-enabled laboratory data integration system and a machine learning pipeline for a major cannabis reseller. Implemented data pipelines with AWS S3, Glue, Athena, and Lambda, while leveraging Amazon Bedrock and Q Business for advanced generative AI and chatbot capabilities.","AWS S3, Glue, Athena, Lambda, Amazon Bedrock, Amazon Q Business, GPT-4","Senior Data Engineer, Data and AI/ML practice","Successfully closed high-value client deals by translating complex business requirements into scalable technical solutions. Developed an abstracted AI tooling framework to quickly connect client databases to Amazon Q Business, significantly reducing time-to-value for Mindex’s future projects and accelerating delivery across multiple engagements."
Quinn Audio Library,"Designed and implemented a scalable generative AI platform for Quinn Audio Library to process and tag large volumes of audio content. Integrated Whisper AI for transcription, orchestrated through Redis as a message broker and containerized with Docker on AWS ECS. Added Mixtral-based tagging models to categorize audio files, with data landing in S3 enriched with metadata for search and discovery. The pipeline, built on FastAPI, processed daily ingestions from the audio API, enabling near-real-time classification of thousands of audio files.","Whisper AI, Redis, AWS S3, AWS ECS, Mixtral, FastAPI, Docker",Senior LLM Engineer,"Automated metadata tagging for high-volume audio content, dramatically reducing manual labeling efforts and enabling rapid search capabilities for downstream consumers."
General Dynamics,"Served as Senior Data and AI Architect at General Dynamics, designing and maintaining a scalable data platform to ingest data from heterogeneous public sources including SAM.gov and USAspending.gov, enabling downstream data science and reporting initiatives. In parallel, led the development of a custom knowledge graph application for an Air Force client, incorporating a vector database for semantic search, knowledge graph modeling for entity relationships, and live API integrations with federal data sources. Integrated LangChain for request routing, LangSmith for tracing and debugging, and built an NER pipeline with spaCy to classify and enrich ingested data. docling was used to handle varied document formats, ensuring comprehensive document parsing and ingestion.","AWS S3, LangChain, LangSmith, spaCy, docling, QDrant, Neo4j, FastAPI, React.js",Senior Data / AI Architect,"Served as Senior Data and AI Architect at General Dynamics, designing and maintaining a scalable data platform to ingest diverse public data sources including SAM.gov and USAspending.gov, enabling multiple downstream analytics and data science projects. Additionally led the development of a custom knowledge graph application for an Air Force client, combining QDrant for semantic vector search with Neo4j for entity relationship modeling. Built API connectors to federal data sources and a user-facing interface with FastAPI and React.js, while leveraging LangChain for request routing and LangSmith for query tracing and debugging. Integrated spaCy-powered NER to classify text, and docling to handle ingestion of varied document types for robust data processing."
Triodide,"Ingested hundreds of clips of social media data from Twitch, Youtube, and TikTok, and created a Roboflow pipeline to be able to extract images, tag different levels and enemies, and extract sentiment from these clips. Designed and implemented a scalable data platform to enable data-driven insights and product decision-making at Triodide. Ingested and processed Steam review data, applying NLP techniques with spaCy and clustering analysis to surface the most frequent user complaints and compliments. Developed an A/B testing framework with AWS AppConfig to drive experimentation, and built a churn analytics pipeline using Amazon SageMaker to predict user disengagement. Integrated all data assets into a consistent platform leveraging S3, Glue, and Athena to support downstream business reporting and product iteration.","AWS S3, AWS Glue, AWS Athena, spaCy, AWS AppConfig, Amazon SageMaker, GPT-4, Roboflow, CLIP",Data Platform Engineer,"Empowered the company to prioritize product improvements based on user sentiment and retention risk, accelerating decision-making and driving more targeted feature development."
Aerial Insights,"Designed and developed a full-stack computer vision application for Aerial Insights to automatically detect damage on roofing shingles from image data. Trained object detection models with YOLOv8, leveraging Roboflow for dataset management and augmentation. Built a scalable backend with FastAPI and Docker to serve predictions, and developed the front-end interface with React.js to allow user interaction and review. Integrated storage and retrieval of data through AWS S3, creating a seamless user experience from upload to damage reporting.","Roboflow, YOLOv8, AWS S3, FastAPI, Docker, Python, React.js",Senior Computer Vision Engineer,"Delivered a production-ready MVP currently being showcased to investors for Series funding, demonstrating scalable, automated damage detection capabilities to modernize the roofing inspection industry."
Home Care Consulting Group,"Designed and implemented an automated insurance claim verification system for Home Care Consulting Group, leveraging generative AI to dynamically generate SQL queries via NLP for complex data validation. Built a robust ingestion framework using docling and Pydantic to parse and join various document formats, with data landing in S3 and queryable through Athena. Integrated LangChain for natural language processing and routing of user requests, enabling rapid, accurate claims verification with a high degree of flexibility.","AWS S3, AWS Athena, LangChain, docling, Pydantic, Python",Senior Data Architect,"Delivered a production system still in use today, significantly reducing manual review times and improving claims validation accuracy for the organization."
ShadowPulse,"Developed a mobile telemetry analysis tool for ShadowPulse, designed to identify potential IMSI catchers (cellular network interception devices) using data collected from Graphite OS–hardened Google Pixel devices. Built a pipeline to ingest and parse radio signal metadata, applying MobileBERT to classify potential suspicious patterns and routing the results to users for real-time awareness. Integrated MapBox for geospatial visualization, and stored analysis data locally using SQLite. Delivered a scalable Android Studio-based application for field security operations.","Graphite OS, MapBox, Android Studio, MobileBERT, SQLite",Senior Mobile App Engineer,"Delivered a prototype currently undergoing extended security team validation and investor review, demonstrating potential to significantly improve detection of rogue cellular interception devices."
StockPilot,"Developed a real-time conversational application for StockPilot using LangChain and Streamlit, enabling users to query and discuss live market events through integrations with SEC.io and Polygon.io data streams. Built a seamless chatbot interface that provided immediate, context-aware responses about trading data and financial filings. Additionally, created a custom LangChain tool plugin for streamlined market data ingestion and semantic analysis, which was accepted as a community contribution prior to LangChain’s refactor.","LangChain, Streamlit, SEC.io, Polygon.io, Python",Senior ML Engineer,"Empowered traders and analysts to gain immediate insights into market activity with a conversational interface, reducing research friction and enhancing decision-making speed."
Ziplingo,"Designed and implemented a predictive churn pipeline for Ziplingo, processing their application data to identify high-risk members prior to disengagement. Built feature engineering workflows with Glue and Athena, and trained a robust churn classification model using SageMaker and XGBoost. Developed QuickSight dashboards to visualize churn risk and prioritize customer outreach campaigns, enabling data-driven retention efforts.","AWS S3, AWS Glue, AWS Athena, Amazon SageMaker, XGBoost, Amazon QuickSight",Senior Data / ML Engineer,"Increased customer retention by 1%, translating to millions of dollars in annual revenue saved through proactive engagement strategies."
JECO,"Designed and implemented a high-volume receipt and invoice validation system for JECO, in partnership with 7-Eleven, to automate the processing of over 60,000 invoices per month. Leveraged AWS Textract to extract structured data from scanned receipts, storing the results in S3 and enabling downstream analysis with Athena. Developed QuickSight dashboards to monitor processing accuracy and operational performance, providing real-time transparency for business stakeholders.","AWS S3, AWS Athena, Amazon QuickSight, Amazon Textract",Senior ML Engineer,"Dramatically reduced manual validation time and improved data accuracy for 7-Eleven’s invoicing workflows, increasing efficiency and scalability across finance operations."